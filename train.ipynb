{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "train_paths = os.listdir('data/train/embedded_abstracts')\r\n",
    "val_paths = os.listdir('data/val/val_embedded_abstract')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data\r\n",
    "- embedded\r\n",
    "- tokenized categories\r\n",
    "- target"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "## Embedded\r\n",
    "embedded_abstract = []\r\n",
    "for path in train_paths:\r\n",
    "    embedded_abstract.append(np.load('data/train/embedded_abstracts/'+path))\r\n",
    "    if len(embedded_abstract) == 5: break\r\n",
    "\r\n",
    "val_embedded_abstract = []\r\n",
    "for path in val_paths:\r\n",
    "    val_embedded_abstract.append(np.load('data/val/val_embedded_abstract/'+path))\r\n",
    "    if len(val_embedded_abstract) == 5 :break\r\n",
    "\r\n",
    "## tokenized\r\n",
    "tokenized = np.load('data/train/tokenized.npy')\r\n",
    "\r\n",
    "## target\r\n",
    "target = np.load('data/train/target.npy')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "embedded_abstract_padded = tf.keras.preprocessing.sequence.pad_sequences(embedded_abstract, padding='post', dtype=float)\r\n",
    "val_embedded_abstract_padded = tf.keras.preprocessing.sequence.pad_sequences(val_embedded_abstract, padding='post', dtype=float)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def get_datasets(embedded_abstract, tokenized, target, batch_size=32):\r\n",
    "    ## INPUTS.abstract\r\n",
    "    embedded_abstract = tf.data.Dataset.from_tensor_slices(embedded_abstract)\r\n",
    "\r\n",
    "    ## INPUTS.categories\r\n",
    "    categories_dataset = tf.data.Dataset.from_tensor_slices(tokenized)\r\n",
    "    \r\n",
    "    ## INPUTS\r\n",
    "    inputs_datasets = tf.data.Dataset.zip((embedded_abstract, categories_dataset))\r\n",
    "\r\n",
    "    ## TARGETS\r\n",
    "    target_dataset = tf.data.Dataset.from_tensor_slices(target)\r\n",
    "\r\n",
    "    ## (INPUTS, TARGETS)\r\n",
    "    datasets = tf.data.Dataset.zip((inputs_datasets, target_dataset))\r\n",
    "    datasets = datasets.padded_batch(batch_size, padded_shapes= (([45, 512], [45]), []))\r\n",
    "    \r\n",
    "    return datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "train_datasets = get_datasets(embedded_abstract=embedded_abstract_padded,tokenized=tokenized, target=target, batch_size=2)\r\n",
    "#val_datasets = get_datasets(embedded_abstract=val_embedded_abstract, tokenized=val_tokenized, target=val_target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "for inputs, targets in train_datasets.take(2):\r\n",
    "    print(f'inputs[abstract].shape = {inputs[0].shape}')\r\n",
    "    print(f'inputs[categories].shape = {inputs[1].shape}')\r\n",
    "    print(f'targets.shape = {targets.shape}')\r\n",
    "    print()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs[abstract].shape = (2, 45, 512)\n",
      "inputs[categories].shape = (2, 45)\n",
      "targets.shape = (2,)\n",
      "\n",
      "inputs[abstract].shape = (2, 45, 512)\n",
      "inputs[categories].shape = (2, 45)\n",
      "targets.shape = (2,)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from model import UseZeroClf\r\n",
    "\r\n",
    "model = UseZeroClf(gru_units = 128, \r\n",
    "                   dense_units = [256, 128], \r\n",
    "                   category_vocab_size = tokenized.max(), \r\n",
    "                   emb_dim = 512)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\r\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n",
    "\r\n",
    "model.compile(optimizer=optimizer, loss=loss_object,\r\n",
    "              metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(\r\n",
    "                monitor='loss', min_delta=0.001, patience=5, verbose=1, restore_best_weights=False\r\n",
    "            )\r\n",
    "\r\n",
    "history = model.fit(train_datasets, epochs=100, callbacks=[early_stop])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "b688f47bcdb1c0e515b44e89a41b569d8d7da665d0b69b19a13e57fad679b628"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}